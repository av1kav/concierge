{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6842b4f7",
   "metadata": {},
   "source": [
    "# Team Project: Concierge, the AI-powered meeting assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea80a7",
   "metadata": {},
   "source": [
    "Forget AI code generation, autocomplete and automated testing – what developers and SMEs really need is less time spent in meetings all day. _Concierge_ is an AI-enabled meeting summarization and action item tracker that ensures key takeaways are recorded, summarized and communicated effectively without the overhead of a dedicated individual. Our service will leverage large language models and video analysis tools to generate meeting minutes, notify stakeholders and communicate achievements, roadblocks and deliverables to the right people at the right time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbd0a1",
   "metadata": {},
   "source": [
    "## Extracting audio from meeting video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67bd03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, we're using an actual meeting from the team at Gitlab - a great example of a typical corporate conference call. \n",
    "video_url = \"https://www.youtube.com/watch?v=qGFoZ8yodc4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53138653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↳ |████████████████████████████████████████████| 100.0%\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/avenugopal/Desktop/UB Work/MGS 636 Applied AI for Managers/Team Project/meeting_video.mp4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytubefix import YouTube\n",
    "from pytubefix.cli import on_progress\n",
    " \n",
    "video = YouTube(video_url, on_progress_callback = on_progress)\n",
    "video.title=\"meeting_video\"\n",
    "video.streams.get_lowest_resolution().download() # Since we only need the audio, this saves time and space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b970ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Running:\n",
      ">>> /Users/avenugopal/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/imageio_ffmpeg/binaries/ffmpeg-macos-aarch64-v7.1 -y -i meeting_video.mp4 -ab 3000k -ar 44100 meeting_audio.mp3\n",
      "MoviePy - Command successful\n",
      "audio saved at: meeting_audio.mp3\n"
     ]
    }
   ],
   "source": [
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_audio\n",
    "\n",
    "audio_path = \"meeting_audio.mp3\"\n",
    "ffmpeg_extract_audio(\"meeting_video.mp4\", audio_path)\n",
    "print(\"audio saved at:\", audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a4ed2",
   "metadata": {},
   "source": [
    "## Audio Transcription using Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b4a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8cd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476f61b",
   "metadata": {},
   "source": [
    "### Direct Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b79db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avenugopal/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"meeting_audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bbc829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, this is Eric Johnson. It's February 18th, 2021, and this is the engineering key review at GitLab. So I've got number four in the\n",
      "agenda, which is a proposal to break up this meeting into four department key reviews. So currently this is engineering, development,\n",
      "quality, security, and UX. Infrastructure and support do their own key reviews already. I have the reasons why increased visibility, able to\n",
      "go deeper, increase the objectivity with which my reports can manage their groups, allow me more time to focus on new markets, and allow me\n",
      "to shift into more of a question asker mode than generating content and answering questions in these meetings. And, but to avoid adding\n",
      "three net new meetings to stakeholders calendars, I propose we do a sort of two month rotation. So month one, development quality go, month\n",
      "two, security and UX would go. How do people feel about that proposal? I think in the group conversations, it's working really well. So I'm,\n",
      "I'm supportive. And this is the smallest thing. Maybe we need four meetings a month. Like it's the biggest apartment. It's super essential,\n",
      "but you propose this. I don't, I could see either way. So let's stick with the proposal. Cool. We'll try it and we'll, we can be flexible. I\n",
      "mean, the development is larger. Maybe they go more frequently or something like that, but we'll see how it, see how it goes. All right. And\n",
      "then I've got number five, which is we've got R and D overall MR rate, and we also have R and D wider MR rate, both as top level KPIs for\n",
      "engineering. So the difference between them in the simplest sense is that R and D wider MR rate includes both community contributions and\n",
      "community MRs. The problems I see with this are that one, the wider MR rate, the one that includes internally and external MRs, it\n",
      "duplicates the overall MR rate, which is. Wait, wait, wait, wait, sorry, sorry, sorry. The wider MR rate should just be external, right? And\n",
      "then overall should be narrow plus wider. Oh yeah. It's like we say the wider community. Right, right, right. Okay. So there's, I'll have to\n",
      "check the taxonomy. Lily, can you, can you confirm SID's, that's, that's, SID's reasoning is my, my understanding as well. Yeah, I believe\n",
      "water MR rate just captures community contributions. Only and no internal. Yeah. And the reason we measure that is that like one of the most\n",
      "likely failure modes is that we lose the community. Yeah. So Eric, where it gets goofy is, is that when you look at a specific team within\n",
      "the company, there could be contributions outside of that that aren't community contributions. They would be viewed community contributions\n",
      "by that group, but effectively they're not from outside the company. So that's why we use wider to kind of reflect that. And narrow is very\n",
      "specific to the team. Are you saying that if someone in plan contributes to verify it's viewed as wider? Not quite that plan, plan and\n",
      "verify are just fine. Yeah. It's when you look at like the development versus infrastructure infrastructure will oftentimes contribute to\n",
      "developments work, but it won't be counted as MRs. Okay. That's a, that's a potential bit of funkiness that we should talk about separately.\n",
      "I didn't have that in my sort of critique of this, but that, that doesn't necessarily make intuitive sense to me. Um, so, so then I think,\n",
      "um, part of my, uh, critique of this can be thrown out because it's not as duplicative as I thought, but I still think there's a problem\n",
      "with R and D wider MR rate, which is, um, this thing doesn't really move in part because it's a, it's a rate. So it feels like the way to\n",
      "drive this up is to specifically drive community authors to contribute more than one MR per month. That's how this moves up because it's a,\n",
      "it's a productivity rate like we use internally. And that doesn't necessarily feel like the right thing. Cause there's scenarios in which\n",
      "this goes up and we've actually got less contributions overall and less contributors overall. Wait, wait, wait, wait, wait a second. So you,\n",
      "you're saying that R and D wider MR rate is number MRs per external contributor. Oh my goodness. That should not be the thing. It should be\n",
      "contributions per GitLab team member. So the country, those, the, the, the, the thing above the division is the external ones. The thing\n",
      "below is the number of team members at GitLab. Is that the case, Lily? I'm checking right now. Um, I think so. It says R and D merge per\n",
      "team member. So unless we start calling people outside the company team member, then it shouldn't be done. Yeah. Yeah. Just clarifying here.\n",
      "So our numerator is community contributions and then our, uh, denominator is, uh, GitLab team members. So it's not per external, um, member.\n",
      "So we, so what we're doing there, Eric is we're, we're not trying to say how many, how many MRs does someone sent? If they send something,\n",
      "we're saying how many MRs from external do we get for the size of our organization? Sorry. Sorry. I have a childhood emergency outside the\n",
      "door. Um, Um, so, so the, maybe explain the context behind this. The context is as we grow as a company, we should make sure we, we keep the\n",
      "community up. Like the logical thing is for the community to flatline and the size of the org to go. And before you know it, you've kind of\n",
      "outgrown the wider community. Yeah. And what I'm seeing is we, we created this pretty sophisticated taxonomy with prefixes and post fixes to\n",
      "talk about these things. But in reality, we've only got two of them and it's, we keep forgetting. We have a hard time discussing this thing.\n",
      "So I'd rather just name them simply. So two names for what they are rather than using the taxonomy, but also like in F I have this proposal\n",
      "of like, what if we just tracked as a KPI, the percentage of total MRs that come from the community over time. And we would see that drop. I\n",
      "love that. I love that. Let's do that instead. Okay. But the, the, the, the thing, the thing why we have this complex thing is because you\n",
      "can game that you want to game that you just produce fewer MRs with the engineers at GitLab. So if you drive that really hard and say, this\n",
      "is your number one goal, it's very easy to achieve. You just tell all your engineers to produce half. Yeah. So we have, we have different\n",
      "metrics to prevent that from happening in the same way that like support SLA is an SAC kind of buttress one another. I think we're, we're\n",
      "robust to that, but simplifying this would make these conversations go. If, if you, as our CTO don't even understand them, we went\n",
      "overboard. So I'm supportive. You don't understand them. And I forgot. And I was agreeing to stuff this morning. I'm like, there's a problem\n",
      "with this. And then you just remind me of the context. So yeah, if I can't hold it in my head. Percentage that come from the community. I\n",
      "love that. It's what all our investors ask about. Let's do that. Okay, cool. So, um, Lily, if you can work with, uh, Mac to make that\n",
      "transition, that would be great. And I'll bold the one that we're talking about. That's Roman or Malthory. Thanks. I'm on the call. Sorry.\n",
      "It was a bit late. Uh, uh, timeline. We do have PIs on the raw number of community MRs and we can make, make the shift, uh, and wider\n",
      "confirming from the definition. I think wider only counts for community. And that's, that's what the definition is. Cool. All right. So, uh,\n",
      "number six, then Christopher. Sorry. I was looking up to see if I had the percentage graph. Cause I think we played around with this at one\n",
      "point and had a draft of that. Um, probably about five months back. If I can remember Lily, um, uh, just an FYI, uh, the month of February,\n",
      "if you were looking at any particular metrics, uh, particularly in development at an MR rate, um, uh, we haven't had updates in four days.\n",
      "Uh, there's a, apparently a lag issue that's been, uh, problematic, uh, for the data team to basically get, uh, updated message, uh, updated\n",
      "metrics. And they're working on that. Okay. I'm sorry, Mick, you got the next one. Um, uh, so yeah, I, I, uh, there's some, some color there\n",
      "on, on the, on mitigation, the, the lag later on, uh, uh, onto, onto seven, uh, we continue. I said FYI, in addition to the KPIs status. I'm\n",
      "sorry, I wanted to just touch on the Postgres replication issue there real quick. Um, I've been trying to get my arms wrapped around it. Um,\n",
      "do we have the right attention to this? This is kind of, Eric, I don't know if you were commenting on, uh, hinting towards this in the last\n",
      "meeting around some of the infrastructure improvements on the product side. I'm just not quite sure whose responsibility it is to focus on\n",
      "getting a handle on some of the constraints we have on replication. Yeah. The, what I was mentioning in the, in the product key review about\n",
      "an hour ago is, I think is sort of like unrelated. Um, and so I think the DRI needs to be your kind of data engineering team, but of course\n",
      "there's a dependency on infrastructure. Cause that's where the data is being piped from. They, they do own that data source. Yeah. I'll say\n",
      "for the replication lag on that slave host where I'm sorry, not the, on the, the, uh, secondary host where, um, uh, the data is being pulled\n",
      "from like infrastructure would be the DRI for that. And so, um, any escalations, but I'll, I'll own those. And I know we have, uh, an action\n",
      "plan for that as far as creating, uh, another dedicated host, um, just for the data team to pull from. Okay. I did actually, uh, I saw that\n",
      "issue and I did talk to Craig Gomes a little bit as well on the database side, just to see if there's some database improvements and I'm\n",
      "still trying to figure out, you know, if it's truly just a dedicated computational sort of resource, a server, or if there's actually some,\n",
      "some database tuning that needs to occur. Do you have a sense of that? There's, so I'd say it's, it's three different things. Um, it's\n",
      "having a dedicated host, um, that doesn't have conflicting query traffic coming from, um, other, other, uh, workloads. Um, there are some,\n",
      "uh, tuning performance or, uh, tuning improvements to be made. And then there's also improvements in, and this is where it does maybe relate\n",
      "a little bit to, um, what the topic was in the, in the last, uh, uh, review, basically the, the overall, uh, demand on the database layer\n",
      "from, um, from.com activities and, and like improving those. So it's, it's definitely not just one of those things. Um, but one of the most\n",
      "specific actions, uh, we're going to take though is separating out and having a dedicated host so that we're just dealing with the profile\n",
      "of the data engineering traffic on there. Um, and, and not having conflicting query, um, uh, conflicting queries, uh, affect the ability to,\n",
      "uh, uh, to update the replication. Steve, I definitely want to partner with you on this one because I don't, I think the, the demand on\n",
      "those databases is only going to increase. It's not going down. And, uh, I think we need to get, I'm still unclear on, uh, where to focus in\n",
      "to get the biggest bang for the buck. I think if the computational resource dedication, that's going to be a good thing, but I'll probably\n",
      "going to squeeze the balloon and then the next, uh, area will, will unearth itself. Okay. And I'll tell you what I'll put into the infra key\n",
      "review for next week. Um, okay. An update on, on this issue. Thank you, Steve. So then back to Mech on seven or yes. Thank you. Uh, and Rob\n",
      "was in the SAS incident this morning as well. Uh, Brian, just, we have the attention there. Uh, number seven, just provide, provide an\n",
      "update on previous conversations. Uh, we're continuing to improve, uh, defect tracking and against SLOs. There is a first iteration PI that\n",
      "we are experimenting to show, uh, percentage of defects meeting, um, the SLOs, uh, key findings. S1s are hovering at 80%, S2 at 60. We've\n",
      "been focused, uh, mostly on S1 and S2 at this point, as as hence why S3 and S4s are lower. Uh, and this, this will likely be the case. We\n",
      "are also in point B are working on the measurement for average, uh, open bugs, uh, age. This would give us a whole picture of what's, what's\n",
      "left. Uh, if the age goes up and down, it'd be, or cleaning the backlog. The average age should go down as well. Uh, there's no PI yet, but\n",
      "I just want to update and, and show that we're on this. Uh, it's not, it's not a, uh, it's not a, it's not a part-time number and on the F1\n",
      "S2. Yeah, I just, I was looking through the, the, the charts and I noted that there was a spike in me and time to close, and just wanted to\n",
      "see if you had any insight into that for us. This is the S2, the F1s looked fine. Yeah, this is where the point B on age and supplemental\n",
      "charts in the backend helps. So I haven't seen a dip in age, nor the count overall. I think it's the latter. We need to dig in a bit deeper\n",
      "in that. And also the data lag, I would like to reevaluate once we have the whole picture, once everything is synced in as well. Christy,\n",
      "you have some insights. Yeah, I'm just wondering if part of this could be the fact that we changed the severity across the board for MRs to\n",
      "S2. And so we may have some older bugs in there that hadn't been addressed because they were at a lower severity. Now we've moved them to\n",
      "S2, and maybe that caused a little spike. That could be the case if we did it in a limited fashion. It won't be a huge volume. We also\n",
      "iterated after that to pin on priority since product owns prioritization. So I wouldn't account it entirely to that. I mean, this isn't the\n",
      "infra key review, but I know that they've gotten backed up on those issues. So if some good portion of those are infra created or related,\n",
      "then that might be lifting it as well. I can take the deeper dig in and then provide an update next time. I think we need extra debug\n",
      "slicing of the data here. Sid, you would like to go to 0.8? Yeah, we are now measuring S1, S2, SLO achievement with closed bugs. But if you\n",
      "then look at the number of bugs, it's exponential growth. And then it would be trivial to achieve 100% SLO achievement if you just look at\n",
      "closed bugs. Even though there would be a major problem in the company, 99% of all bugs are overdue. As long as I only close ones that are\n",
      "still within the SLO, I'll have great achievement. So I think we shouldn't be looking at the closed bugs. I think we should be looking at\n",
      "open bugs. The entire population or percentage of those is within the SLO time. I think we're doing it the wrong way. Thanks for the\n",
      "feedback, Sid. Hence why we wanted to have the average age to measure what's outside in the open. We can make this iteration to also measure\n",
      "focus on the age of all opened, including open bugs. This is all something I have discussed with Christopher in the next iteration as well.\n",
      "And we're happy to adjust. So the key... Yes, go ahead. So average age would get closer to it. It's not what I'm proposing. What I'm\n",
      "proposing is of the open bugs, what percentage is outside of SLO. So display it as a percentage. You do now, just do it about the open bugs,\n",
      "not the closed ones. Ah, got it. Okay. The exceeding SLO for open bugs. Yeah. Or open bugs that are within SLO. So you have a chart that\n",
      "should go up and to the right, like everything else. Sounds great. We can take it to the next data metrics work stream to deliver this.\n",
      "Cool. Thanks. It's a little tricky, Mac. You'll have to figure out how to... Because we like to be able to have charts that we can\n",
      "historically reconstruct if we need to. So when tickets close out, you need to go through their history to figure out at this time when it\n",
      "was open, did it breach the SLO or not? That's a good point. This might be much harder computationally. So I totally respect if we can do it\n",
      "for that reason. Cool. Nine, Craig. Yeah. I just wanted to ask the team, like I went through all the key meeting metrics, everything looked\n",
      "in line with prior periods and looked good. Is there anything the team wants to call out, especially that we should be watching? Yeah, I'll\n",
      "call out Sus. So the good news is in Q4, we had our smallest decline over several quarters. So we only went down by a tenth of a point. The\n",
      "quarter previous was 0.6 or six tenths of a point. And the quarter before that was a full point. So we see this as an improvement, even\n",
      "though it was still a decline, but it's still a decline. Obviously, we want this actually tracking in an upward direction. We also don't\n",
      "have enough data to know whether or not this is an actual real trend up. So I'm optimistic. I think this is a good thing. We have had a much\n",
      "keener focus on Sus over the past several quarters. So that's why I think, okay, the work that we've done, I think, actually is catching up\n",
      "and getting noticed in Sus, but we got to keep an eye on it. We can't, we cannot assume that that's the case. Yeah. And the bug discussion\n",
      "above just kind of points out that like we have an underlying problem right now in our metrics measurement. So if we change the measurements\n",
      "to reflect that, then hopefully we're in good shape. If we don't, and we flatline and address it so that we flatline open S1s and S2s, you\n",
      "will see a temporary jump in and above SLOs as we clear out that backlog over that period of time. And I have point C, which is similar to\n",
      "infrastructure. We need to get more security work prioritized. We're hearing that from the team, but neither that problem or that activity\n",
      "is sort of currently reflected in our security metrics. So we have some work to do long-term to make sure the, we see things like that in\n",
      "the metrics and the measurements that we're making. So back to you, Sid, 10. Yeah. The narrow MR rate seems significantly below target and\n",
      "maybe I, I hope that it would bounce back from December. I think it bounced back, but not back on target. Any, any context there? What's,\n",
      "what's going on? Yeah. So with family and friends days, we actually had some heavier vacation days in January than we historically have. One\n",
      "thing to note is that we are actually at a higher MR rate. If you look at, if you go back the last 18 months, we're actually at a higher and\n",
      "narrow MR rate than we were back in each month this year. So if you compare October to October, November to November, and January, I'm\n",
      "sorry, October, November, December, and January, comparatively to last year, what you'll find is we're between a half point and a 1.5 MR\n",
      "rate above where we were in the month of previous year. That's, that's great context. Thank you, Christopher. Good work. Yeah. So the\n",
      "expectation is, is that, uh, February is a short month. Uh, we were at, I think 16 work days with friends and family day and, uh, other\n",
      "things. Um, obviously us having car ridages in Texas doesn't help things either for the folks who are working in Texas. Um, but hopefully\n",
      "the rest of the team is being effective. Um, I was hoping to see a better result right now, but with four or five days, particularly around\n",
      "a release week, um, that's usually when we see, do, do see a little bit higher activity. Um, so that's not accounted for yet. Um, but you\n",
      "know, March is, March is when I'm expecting to kind of see a real rebound, much like we did last year. Awesome. Thanks. The other context\n",
      "I'll give is, um, we now do time series targets. So when we change the target, you'll see that reflected in the line. So if we were to look\n",
      "back historically here, the goal here was actually lower and Christopher was ambitious and we kept raising and we kept meeting that. So it\n",
      "should stair step here and we could go back and reconstruct that if we really wanted to. And then ignoring the sort of the, the seasonal dip\n",
      "here, um, we raised it to, I think 11. And then we realized we're kind of hitting that point of diminishing returns and the right thing to\n",
      "do business wise. And this is in our FY22 direction is hold the line at productivity, but start to raise other things related to quality,\n",
      "security, availability, and, and whatnot. Um, so that's kind of why you're seeing this bump is we raised it and we realized, okay, that's\n",
      "not the, we shouldn't raise it anymore. And we brought it back down to 10. So 10 will be the target going forward. I'm going to try to get\n",
      "better at a lot of other things while preventing this from dipping. Cool. And I just want to call out that it's not necessarily like M\n",
      "higher MR rates should also help to address security and quality and other things because you're more productive. So you can fix more\n",
      "things. So it's not necessarily opposite, but I, I, I agree with, let's hold the line. 10 is, 10 is a great number and, uh, and focus on\n",
      "other, uh, indicators to improve. That makes a ton of sense. Cool. Well said. All right. That's it for the agenda. Anyone want to vocalize\n",
      "anything else? Great. Well, thanks everybody. And I'm going to go check on my four-year-old and see if she got what, whatever she needed. So\n",
      "cheers and talk soon. Thanks, Eric.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for line in textwrap.wrap(result[\"text\"], width=140):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0eea54",
   "metadata": {},
   "source": [
    "### NLP using Spacy: Keyword/Keyphrase extraction for contextual clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fad02d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pprint import pprint \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "def initialize_matcher(phrase_dict):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "    for name, phrase_list in phrase_dict.items():\n",
    "        patterns = [nlp(text) for text in phrase_list]\n",
    "        phrase_matcher.add(name, None, *patterns)\n",
    "        print(f\"Added {len(phrase_list)} phrases to PhraseMatcher '{name}'.\")\n",
    "    return phrase_matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 phrases to PhraseMatcher 'engineering'.\n",
      "Added 9 phrases to PhraseMatcher 'leadership'.\n",
      "Added 2 phrases to PhraseMatcher 'sales'.\n"
     ]
    }
   ],
   "source": [
    "phrases = {\n",
    "    'engineering': ['engineering', 'engineer', 'code', 'release', 'developer', 'infra'],\n",
    "    'leadership': ['senior leadership', 'c-suite', 'cxo', 'ceo', 'cto', 'ciso', 'manager','upper management', 'stakeholders'],\n",
    "    'sales': ['sales target', 'client-facing']\n",
    "}\n",
    "\n",
    "corpus = str(result[\"text\"])\n",
    "phrase_matcher = initialize_matcher(phrases)\n",
    "\n",
    "doc = nlp(corpus)\n",
    "output = {domain: [] for domain in phrases.keys()}\n",
    "for sent in doc.sents:\n",
    "    for match_id, start, end in phrase_matcher(nlp(sent.text)):\n",
    "        output[nlp.vocab.strings[match_id]].append(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f56d5148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'engineering': [\"It's February 18th, 2021, and this is the engineering key \"\n",
      "                 'review at GitLab.',\n",
      "                 'So currently this is engineering, development, quality, '\n",
      "                 'security, and UX.',\n",
      "                 \"And then I've got number five, which is we've got R and D \"\n",
      "                 'overall MR rate, and we also have R and D wider MR rate, '\n",
      "                 'both as top level KPIs for engineering.',\n",
      "                 'Um, and so I think the DRI needs to be your kind of data '\n",
      "                 \"engineering team, but of course there's a dependency on \"\n",
      "                 'infrastructure.',\n",
      "                 \"Um, but one of the most specific actions, uh, we're going to \"\n",
      "                 'take though is separating out and having a dedicated host so '\n",
      "                 \"that we're just dealing with the profile of the data \"\n",
      "                 'engineering traffic on there.',\n",
      "                 \"And I'll tell you what I'll put into the infra key review \"\n",
      "                 'for next week.',\n",
      "                 \"I mean, this isn't the infra key review, but I know that \"\n",
      "                 \"they've gotten backed up on those issues.\",\n",
      "                 'So if some good portion of those are infra created or '\n",
      "                 'related, then that might be lifting it as well.',\n",
      "                 'Um, I was hoping to see a better result right now, but with '\n",
      "                 'four or five days, particularly around a release week, um, '\n",
      "                 \"that's usually when we see, do, do see a little bit higher \"\n",
      "                 'activity.'],\n",
      " 'leadership': ['And, but to avoid adding three net new meetings to '\n",
      "                'stakeholders calendars, I propose we do a sort of two month '\n",
      "                'rotation.'],\n",
      " 'sales': []}\n"
     ]
    }
   ],
   "source": [
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eea7d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': '1 new documents added successfully'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8002/rag/add_documents/\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "meeting_information  = [\n",
    "    {\n",
    "        \"text\": result['text'],\n",
    "        \"source\": \"Zoom Meetings (Recorded)\",\n",
    "        \"date\": \"2024-05-10T14:23:45.123Z\",\n",
    "        \"meeting_id\": \"8a7fd89912e\",\n",
    "        \"participants\": [\"Sid\", \"Eric Johnson\"],\n",
    "        \"tags\": [domain for domain in output.keys() if output[domain]]\n",
    "    }\n",
    "]\n",
    "response = requests.post(url, headers=headers, data=json.dumps(meeting_information))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5cbc4075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\":\"Based on the transcript, Sid’s action items are to focus on improving key indicators related to productivity and quality while holding the line at a target of 10 for MR (Monthly Rate). He also needs to address security, quality, availability, and other factors to prevent the narrow MR rate from dipping. Essentially, Sid needs to prioritize maintaining current productivity levels while proactively addressing potential issues to ensure long-term success and avoid a downward trend in MR.\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \"http://localhost:8002/rag/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"What are action items for Sid?\", \"source\": \"Zoom Meetings (Recorded)\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbcb9e",
   "metadata": {},
   "source": [
    "### Transcription with Speaker Diarization (requires compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a055566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avenugopal/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m diarization_pipeline \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyannote/speaker-diarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_auth_token\u001b[38;5;241m=\u001b[39mHF_TOKEN)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 1: Diarize\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m diarization \u001b[38;5;241m=\u001b[39m \u001b[43mdiarization_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeeting_audio.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: Cut and transcribe each speaker segment\u001b[39;00m\n\u001b[1;32m      9\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/pyannote/audio/core/pipeline.py:327\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, file, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessors\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    325\u001b[0m     file \u001b[38;5;241m=\u001b[39m ProtocolFile(file, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessors)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/pyannote/audio/pipelines/speaker_diarization.py:523\u001b[0m, in \u001b[0;36mSpeakerDiarization.apply\u001b[0;34m(self, file, num_speakers, min_speakers, max_speakers, return_embeddings, hook)\u001b[0m\n\u001b[1;32m    520\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinarized_segmentations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_exclude_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m     hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings)\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m#   shape: (num_chunks, local_num_speakers, dimension)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/pyannote/audio/pipelines/speaker_diarization.py:348\u001b[0m, in \u001b[0;36mSpeakerDiarization.get_embeddings\u001b[0;34m(self, file, binary_segmentations, exclude_overlap, hook)\u001b[0m\n\u001b[1;32m    345\u001b[0m mask_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(masks)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# (batch_size, num_frames) torch.Tensor\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m embedding_batch: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwaveform_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_batch\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# (batch_size, dimension) np.ndarray\u001b[39;00m\n\u001b[1;32m    353\u001b[0m embedding_batches\u001b[38;5;241m.\u001b[39mappend(embedding_batch)\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/pyannote/audio/pipelines/speaker_verification.py:374\u001b[0m, in \u001b[0;36mSpeechBrainPretrainedSpeakerEmbedding.__call__\u001b[0;34m(self, waveforms, masks)\u001b[0m\n\u001b[1;32m    370\u001b[0m wav_lens \u001b[38;5;241m=\u001b[39m wav_lens \u001b[38;5;241m/\u001b[39m max_len\n\u001b[1;32m    371\u001b[0m wav_lens[too_short] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    373\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m embeddings[too_short\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/speechbrain/inference/classifiers.py:112\u001b[0m, in \u001b[0;36mEncoderClassifier.encode_batch\u001b[0;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[1;32m    110\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mcompute_features(wavs)\n\u001b[1;32m    111\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mmean_var_norm(feats, wav_lens)\n\u001b[0;32m--> 112\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[1;32m    114\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mmean_var_norm_emb(\n\u001b[1;32m    115\u001b[0m         embeddings, torch\u001b[38;5;241m.\u001b[39mones(embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    116\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/speechbrain/lobes/models/ECAPA_TDNN.py:548\u001b[0m, in \u001b[0;36mECAPA_TDNN.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# Multi-layer feature aggregation\u001b[39;00m\n\u001b[1;32m    547\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(xl[\u001b[38;5;241m1\u001b[39m:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 548\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmfa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Attentive Statistical Pooling\u001b[39;00m\n\u001b[1;32m    551\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masp(x, lengths\u001b[38;5;241m=\u001b[39mlengths)\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/speechbrain/lobes/models/ECAPA_TDNN.py:85\u001b[0m, in \u001b[0;36mTDNNBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes the input tensor x and returns an output tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)))\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/speechbrain/nnet/CNN.py:455\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadding must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 455\u001b[0m wx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze:\n\u001b[1;32m    458\u001b[0m     wx \u001b[38;5;241m=\u001b[39m wx\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UB Work/MGS 636 Applied AI for Managers/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def extract_audio_chunk(audio_path, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Extract a segment of an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the input audio file (.wav, .mp3, etc.)\n",
    "        start_time (float): Start time in seconds.\n",
    "        end_time (float): End time in seconds.\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: Extracted audio chunk.\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    start_ms = start_time * 1000\n",
    "    end_ms = end_time * 1000\n",
    "    chunk = audio[start_ms:end_ms]\n",
    "    return chunk\n",
    "\n",
    "HF_TOKEN = \"hf_YOUR_TOKEN_GOES_HERE\"\n",
    "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=HF_TOKEN)\n",
    "\n",
    "# Step 1: Diarize\n",
    "diarization = diarization_pipeline(\"meeting_audio.mp3\")\n",
    "\n",
    "# Step 2: Cut and transcribe each speaker segment\n",
    "i = 1\n",
    "for segment, track, speaker in diarization.itertracks(yield_label=True):\n",
    "    i+=1\n",
    "    audio_chunk = extract_audio_chunk(\"meeting_audio.mp3\", segment.start, segment.end)\n",
    "    text = model.transcribe(audio_chunk)\n",
    "    print(f\"Segment {i}:: [{speaker}]: {text}\")\n",
    "    if i >= 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
